{
  "students": [
    {
      "slackId": "U078U88QTN3",
      "name": "Arjun Dahal",
      "slug": "arjun-dahal",
      "photo": "arjun.jpeg",
      "updates": [
        {
          "date": "2026-01-22",
          "content": "**Arjun Dahal:**\n\nHi,\n\nMilestone: TabChange submission ( Jan 22, Internal Deadline ), Fairness diagnosis and Repair project\n\nAction Items:\n\n\n1. I have finalized a couple of baselines. \n2. I am working on adapting the baselines for our use case.\n\n\nResult/Findings:\nI do not have any new notable findings.\n\nIssues/Questions:\nThe adaptation of baselines to find counterfactuals in attributes isn't straightforward. I am currently fixing the issues related to the adaptation one by one.\n\nI am a little under the weather this week.",
          "placeholder": false
        },
        {
          "date": "2026-01-15",
          "content": "**Arjun Dahal:**\n\nHello Everyone\n\nMilestone: TabChange submission ( Jan 22, Internal Deadline ), Fairness diagnosis and Repair project\n\nAction Items:\n\n\n1. I am working towards getting results for the baselines of TabChange. I was working on adapting a couple of baselines this past week. \n2.  I am conducting a literature review on not using sensitive information during prediction in the bias removal project.\n\n\nResult/Findings:\nI do not have any new notable findings; the findings have already been shared in Slack. I will update my research channel once I have more updates.\n\nIssues/Questions:\nI have no issues at the moment.\n\n**Replies:**\n\n**Dr. Lei:**\nHave a to-do list with target dates on what needs to be done and by when for you to make the submission. Then work on each item, and finish it off, with discipline and commitment.\n\n---",
          "placeholder": false
        },
        {
          "date": "2026-01-08",
          "content": "**Arjun Dahal:**\n\nHello Everyone\n\nMilestone: Fairness diagnosis and Repair project, TabChange submission ( Jan 22, Internal Deadline )\n\nAction Items:\n\n\n1. I had been working on removing the sensitive information from the NN to improve the fairness. Now, I am pivoting towards not using the information during prediction. I am conducting a literature review on this. \n2. I am working towards getting results for the baselines of TabChange.\n\n\nResult/Findings:\nI do not have any new notable findings; the findings have already been shared in Slack. I will update my research channel once I have more updates.\n\nIssues/Questions:\nI have no issues at the moment.",
          "placeholder": false
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-11",
          "content": "**Arjun Dahal:**\n\nHello Everyone\n\nMilestone: Baseline on Fairness diagnosis and Repair, TabChange KDD submission\n\nAction Items:\n\n\n1.  I have been reading the paper in more depth for whitebox fairness testing that focuses on fault localization and repair.\n2. I am setting up the experiment record the activations in the second last layer of the neural network for different genders. \n\nOnce I have that, I will look into whether they are separable ( linearly or non-linearly ). MMD loss would be used to match these two distributions for Male and Female, along with the classification loss at the last layer. The network would be retrained, and the changes in fairness and accuracy would be measured.\n\nResult/Findings:\nI do not have notable findings. I am working on this experiment to get started and will improve on it. I will get the results for this setup by tomorrow afternoon.\n\nIssues/Questions:\nThe second last layer has all the necessary information from the network. The neurons are responsible for prediction as well as for biases. The literature progresses from identifying bias layers ( and bias neurons within this layer ) to finding bias neurons scattered throughout the layer. Only focusing on this layer might result in a higher drop in accuracy.",
          "placeholder": false
        },
        {
          "date": "2025-12-05",
          "content": "**Arjun Dahal:**\n\nHello Everyone\n\nMilestone: Baseline on Fairness diagnosis and Repair (Dec 11), TabChange KDD submission\nAction Items:\n\n\n1. Prepared the action plan based on reviews of FSE, had feedback from the Stanford Paper reviewer, made changes to the action plan accordingly\n2. Carried out the literature review for the whitebox fairness testing that focuses on fault localization and repair \n\n\nI have initial draft of both of these documents, and I am improving upon it.\n\nResult/Findings:\nI do not have notable findings.\n\nIssues/Questions:\nI have no issues at the moment.\n\nI will present a new idea session for a paper that has whitebox access and works on fault localization and repair.\n\n**Replies:**\n\n**Dr. Lei:**\nCan you share the feedback from the Stanford Paper reviewer?\n\n---\n\n**Arjun Dahal:**\nSure Professor.\n\n---",
          "placeholder": false
        },
        {
          "date": "2025-10-28",
          "content": "**Arjun Dahal:**\n\nHello Everyone,\n\nMilestone: propose a revised end-to-end approach for efficient search for discriminatory pairs in latent space ( Oct 31 )\n\nAction Items Completed:\nUsing Bayesian Optimization in the process.\nExplored different sections of our steps to see if they can be improved.\n\nResults/Findings:\nGaussian Process Regressor with UCB acquisition function can be a starting point for the search.\nBayesian Optimization works best with a continuous latent space. This is particularly important in our case, since other discriminatory instances might make a cluster around the existing discriminatory instance (representative). I have been looking into the feasibility of using Wasserstein autoencoders for our case. There seems to be tradeoff of proximity of counterfactuals when we use this autoencoder for searching the discriminatory latents.\n\nIssues:\nI do not have any issues at the moment.\nI will discuss more on my channel regarding the potential issues and solutions in my channel in coming days.\n\n**Replies:**\n\n**Dr. Lei:**\nBayesian optimization is typically used when the actual target is very expensive to evaluate. Also, it requires a prior, which is difficult to justify for fairness testing.\nI suggest you put a target date (perhaps at most one or two weeks), and if you don't have a breakthrough by that time, move on to other research problems.\n\n---",
          "placeholder": false
        },
        {
          "date": "2024-11-01",
          "content": "**Arjun:**\n\nHello everyone,\n\n**Milestone:** Complete the baseline approach for Fairness Testing\n\n**Action items completed:**\n- Implemented Conditional VAE for data generation\n- Fixed issues with KL divergence\n- Obtained metrics for Reconstruction Loss vs Number of Embedding Dimension\n\n**Next steps:**\n- Complete the implementation of Conditional VAE\n- Review all the Fairness definitions\n\n**Replies:**\n\n**Dr. Lei:**\nSounds good. Keep up with the good work.\n\n---",
          "placeholder": false
        },
        {
          "date": "2024-10-25",
          "content": "**Arjun:**\n\nHi Group,\n\n**Milestone:** Baseline for Fairness Testing | Nov 12\n\nI am working on making changes on the existing library of VAE to incorporate the combinatorial sampling in its latent space.\n\nI will briefly discuss the findings this Friday.",
          "placeholder": false
        }
      ]
    },
    {
      "slackId": "U079VPWQM8X",
      "name": "Saif Uddin Mahmud",
      "slug": "saif-uddin-mahmud",
      "photo": "saif-pic.jpg",
      "updates": [
        {
          "date": "2026-01-27",
          "content": "**Saif Uddin Mahmud:**\n\nWeekly update\nHello everyone,\n\nMilestone: ICSME 2026 -- Internal deadline needs adjustment with new leads guidance\n\nAction Items:\n\n\n- Shared a written report with the project lead summarizing prior work and rationale.\n- Implemented an initial teacher-forcing setup at the attention level to support more controlled comparisons.\n\nResult:\n\n\n- no experimental results to report yet.\n\nQuestion/Issue:\n\n\n- Awaiting feedback and next assigned tasks.",
          "placeholder": false
        },
        {
          "date": "2026-01-20",
          "content": "**Saif Uddin Mahmud:**\n\nHello,\n\nWeekly update - 1/20/2026\n\n**Milestone: **ICSME 26 | Internal Deadline Feb 16\n\n**Action Items:**\nI spent time reviewing relevant background work on attention head interpretability (Vig 2019; Elhelo & Geva 2025; Nam et al. 2025), mainly to understand how similar work handles controlled comparisons and analysis methodology. In parallel, I started drafting the background/related work section while experiments are running.\n\n**Results:**\nThe background review helped clarify how important strict experimental control is for making any claims about attention behavior, which aligns well with the feedback I received. There is no concrete model-level findings yet.\n\n**Issues:**\nMain constraint right now is still compute and model availability for larger models, so progress on large-scale experiments may be a bit staggered. Using this time to solidify the methodology and writing instead.",
          "placeholder": false
        },
        {
          "date": "2026-01-14",
          "content": "No update was provided for the week ending 2026-01-14.",
          "placeholder": true
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-11",
          "content": "**Saif Uddin Mahmud:**\n\nHello group,\n\nMilestone: need to discuss\n\nAction Items\nI set up the full activation-capture pipeline using GPT-2 within TransformerLens to prepare for internal behavior analysis across secure vs. general prompts.\nThe secure and general prompt structure is now working end-to-end, and I validated the activation logging across all layers and tokens.\nI also drafted a  written version of the experimental design document.\nDataset integration is planned next, once the pipeline stabilizes, I will transition from sample prompts to the custom C-code dataset.\nResults\nThe activation-difference and attention-difference extraction pipeline is now functioning correctly on prototype prompts.\nNo model-level findings yet, but the infrastructure for analysis is now stable and ready for scaling.\nIssues\nTransformerLens required several adjustments for handling variable sequence lengths during autoregressive decoding, but these issues are now resolved.\nStill testing on GPT-2 until the full environment for Qwen2.5 / Llama-70B / Gemma-27B is configured, as these models require significant compute and loading considerations.\nDataset integration is pending—current experiments use placeholder prompts until model loading and compute availability are finalized.",
          "placeholder": false
        },
        {
          "date": "2025-11-20",
          "content": "**Saif Uddin Mahmud:**\n\nHi group,\n\n**Milestones**\n\n\n- **Milestone 1:** Complete analysis from all models — Nov 14 (I will be able to share the results on Monday – Nov 24)\n- **Milestone 2:** Complete full project — Nov 30\n\n**Action Items**\n\n\n- Record changes in LLM behavior *original* vs the *secure* prompt and track how these propagate through different model components\n\n\n- generated a small subset with secure prompt. Model internal analysis yet to start.\n\n\n- Demo at least one probing tool to guide the next discussion.\n\n\n- I explored LIT and LLM Probe. I am yet to fully understand how they are actually working\n\n\n- Check if there is any existing work on analyzing the internal behavior of LLMs for secure code generation\n\n\n- Shared and discussed one highly relevant paper that focuses on code correctness\n\n\n- Explore probing techniques across neuron, layer, and model levels to identify suitable methods for our vulnerability-focused analysis -- In progress\n- Check whether these methods reveal differences in internal metrics (e.g., activation patterns, specialization scores) - Yet to start\n- Discuss and decide the appropriate abstraction level for probing (neurons, circuits, layers, or full-model behavior) – yet to start\n\n\n**Results**\n\n\n- No report-able result for this week. all work is in progress.\n\n**Issues**\n\n\n- our first issue is to identify suitable probing tool that aligns with our study. At this stage, I am looking for tools that come out of the box so that we can directly plug-and-play for our study.\n- We do not have sufficient data to train a probing classifier. After discussing with Fadul, we agreed that a good starting point would b the 30 vulnerable samples generated from general prompt and 30 samples from secure prompt. In any case, we first start with plug-and-play options.",
          "placeholder": false
        },
        {
          "date": "2025-10-30",
          "content": "**Saif Uddin Mahmud:**\n\nHi group,\n\n**Milestone 1: Complete the analysis from all the models (both proprietary and free) Nov 14**\n**Milestone 2: Complete the project Nov 30**\n\n**Action Items:**\nI have completed 3rd round of code generation using the three free models. the dataset now contain in total 7.5K generated code (~850 instruction, 3 model, 3 passes) using OpenRouter. I am analysing the false positive rates and the variance of the error distribution by comparing the results from Clang-tidy and Cppcheck.\n\n**Next Actions:**\n\n\n- Compare with any of the frontier models (need to estimate the cost first) --  I am planning to use gpt5-mini by this weekend and haiku 3.5 by next week\n- Find ways to analyze performance on certain types of tests i.e test cases for Functional Requirements, Non Functional Requirements\n\n\nResults: I am still working on the false positive rates. yet to find a meaningful result Issues\n\nIssues: No new issues at this moment.",
          "placeholder": false
        },
        {
          "date": "2024-11-01",
          "content": "**Saif:**\n\nHello group,\n\n**Milestone:** Complete Literature Survey - Dec 5\n\nAction items:\n- Last week, I have summarized three papers for the literature survey:\n  - Python code smell detection using machine learning\n  - An Empirical Study of Code Smells in Transformer-based Code Generation Techniques\n  - Voting Heterogeneous Ensemble for Code Smell Detection\n\n**I will present my current findings to the group on the following Friday (11/8).**\n\n**Replies:**\n\n**Dr. Lei:**\nIf I remember correctly, I suggested you make a schedule towards the paper submission. Can you share the schedule?\n\n---\n\n**Saif:**\nThank you Dr. Lei. I will prepare the schedule and share it by Wednesday.\n\n---",
          "placeholder": false
        }
      ]
    },
    {
      "slackId": "U079MEWK5TK",
      "name": "Fadul Sikder",
      "slug": "fadul-sikder",
      "photo": "fadul.jpg",
      "updates": [
        {
          "date": "2026-01-28",
          "content": "No update was provided for the week ending 2026-01-28.",
          "placeholder": true
        },
        {
          "date": "2026-01-20",
          "content": "**Fadul Sikder:**\n\nWeekly Research Update \nProgress\n This week, I focused primarily on writing and refining the research experiments, with the main objective of improving the evaluation results compared to the current baseline. \n-One key change I am making is to use ESBMC (a bounded model checker) to re-evaluate the third stage of the experimental pipeline, instead of relying solely on symbolic execution. \n-I also continued updating parts of the paper, though progress has been slower than expected. \n-My current target is to complete a full version by Sunday and share it with the group before the Thursday submission deadline. \n\nIssues / Observations\n After revisiting the literature and conducting manual analyses to better understand what signals GNN explanations provide for path generation, I identified several key issues \n1. Limitations of Existing GNN-Based Vulnerability Models \n\n\n- Prior works such as Devign and its successors (e.g., DeepWukong, ReVeal, FUNDED) focus on function-level vulnerability prediction. \n- These models operate on function-level code property graphs(subset like ast, cfg, pdg, combination of some), largely because such graphs are easier to construct with Joerns. \n- They do not handle whole-program analysis, where multiple functions interact. After extensive literature search I am confident that with c/c++ programs their are no GNN based works that do vulnerability predictions on whole program all existion work does it on vulnerable function level. \n\n2. Graph Construction for Whole Programs \n\n\n- For whole-program analysis, a call graph is required to represent connections between functions. \n- In my current dataset, using Devign’s original graph construction approach requires introducing call edges between functions. \n- This raises an important question: Can we still claim this as a Devign-based approach if we modify the graph construction to support multi-function programs \n\n3 . Disconnected Graph Issue in Devign \n\n\n- Upon deeper inspection of the Devign implementation, I realized that although graphs are constructed using their code, the resulting structure consists of multiple disconnected subgraphs, one per function. \n- These subgraphs are not connected to each other, which likely introduces limitations for explanation-guided path exploration. \n- To address this, I am currently modifying the Devign graph construction to connect functions via call edges and retraining a new GNN tailored to this setting. \n\n4. Noise Introduced by Line-Level Abstraction \n\n\n- Another limitation arises when aggregating explanation scores to the line level. \n- This abstraction step may introduce significant noise, potentially obscuring the signal we expect explanations to provide. \n- I am preparing additional concrete examples to demonstrate how this noise affects path generation and will present them in today’s meeting. \n\nNext Steps \n\n\n- Finalize the modified Devign-style graph construction and retrain the GNN. \n- Re-run experiments using ESBMC for third-stage evaluation. \n- Complete and circulate a full draft of the paper by Sunday.",
          "placeholder": false
        },
        {
          "date": "2026-01-14",
          "content": "No update was provided for the week ending 2026-01-14.",
          "placeholder": true
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-17",
          "content": "No update was provided for the week ending 2025-12-17.",
          "placeholder": true
        },
        {
          "date": "2025-12-05",
          "content": "**Fadul Sikder:**\n\nLet me start one.\n\n**Replies:**\n\n**Pujan Budhathoki:**\nyeah, please add me as well.\n\n---",
          "placeholder": false
        },
        {
          "date": "2025-10-30",
          "content": "**Fadul Sikder:**\n\nProgress\n\n\n- Adapted the new LLM-synthetic dataset to our pipeline and modified the final stage to run on the whole program (instead of a test harness).\n- Completed implementation changes and debugging to obtain an initial result set; began correctness analysis (ongoing).\n- Generated test cases for 100 synthetic programs produced by an LLM.\n- - I also been working on to a draft paper. I would send the paper draft to the group before today’s meeting in next couple of hours.\n\nResults\n\n\n- 16/100 vulnerabilities were triggered by the generated test cases.\n- 8/16 were buffer overflows; the remainder included stack overflow and double free.\n\nVerification Caveats\n\n\n- The synthetic dataset provides only yes/no vulnerability indicators—no CWE labels.\n- For verification, I implemented checks for double free, use-after-free, stack overflow, and buffer overflow.\n- Practically, I verify triggers by observing memory-corruption symptoms or signals at compile time with \"Memory-Safe Compilation\" or during execution with AddressSanitizer (ASan) enabled. However, without CWE labels, confirming the remaining 84/100 is challenging—there may be silent data corruption, undefined behavior, or true segmentation faults deeper in program state.\n\nMain Issue\n\n\n- Verification methodology: Without CWE-level labels, it’s unclear whether a non-crashing input truly fails to trigger the intended vulnerability or is targeting a different class. How to write a verifier in terms of whats is going wrong.",
          "placeholder": false
        },
        {
          "date": "2024-11-01",
          "content": "**Fadul:**\n\nHi everyone,\n\nThis week I have been working on:\n- Finalizing the dataset for smart contract vulnerability detection\n- Running experiments with different LLM models\n- Preparing presentation for the research group meeting\n\n**Next week:**\n- Continue with model fine-tuning\n- Start writing the methodology section\n\nLooking forward to discussing this in Friday's meeting.",
          "placeholder": false
        }
      ]
    },
    {
      "slackId": "U079MEEBRAM",
      "name": "Krishna Khadka",
      "slug": "krishna-khadka",
      "photo": "krishna.jpeg",
      "updates": [
        {
          "date": "2026-01-22",
          "content": "**Krishna Khadka:**\n\nHi Group,\nMilestone:\n\n\n1. Proposal\n\nAction Items:\n\n\n1. Last week, I was working on Shovon's IJCAI submission, specifically with experiments and paper writing. \n2. I'm exploring three possible directions for my proposal:\n\n\n1. Explanation of Recommender systems.\n2. Explanation of Vision Language Models.\n3. Expansion of ABLE as global surrogate model.\n\n     Currently, I 'm doing literature review and specifically looking for problems to solve in the domain. I will share my findings in few days in my slack channel on which direction look more promising.\nResults:\n\n\n1. No results yet.\n\nIssues:\n\n\n1. No issues as of now.\n\n**Replies:**\n\n**Dr. Lei:**\nGood job on making the submission. As you do the literature, for each direction, try to be specific about the problem from the user perspective, the challenges from the technical perspective, the key insights to address each challenge, and what are the research gaps. These questions will help you decide which direction to go deep.\n\n---",
          "placeholder": false
        },
        {
          "date": "2026-01-15",
          "content": "**Krishna Khadka:**\n\nHi Group,\nMilestone:\n\n\n1. IJCAI Submission jan 19\n\nAction Items:\n\n\n1. Over the break we discussed on how to improve the approach in terms of training stability and remove the cyclic dependency.\n2. Designed experimental section. Researched on existing baselines and implement them.\n3. Working on paper writing and improving.\n\nResults/Finding:\n\n\n1. The experimental results show that our approach results in better agreement compared to 5 baseline.\n\nIssues:\n\n\n1. No big issues.\n\n**Replies:**\n\n**Dr. Lei:**\nGreat to hear about the results, and glad to see some real collaboration between you and Shovon.\nAlso, now it is time for you to start thinking/discussion about the next project.\n\n---\n\n**Krishna Khadka:**\nsure Dr. Lei. Thank you\n\n---",
          "placeholder": false
        },
        {
          "date": "2026-01-14",
          "content": "No update was provided for the week ending 2026-01-14.",
          "placeholder": true
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-11",
          "content": "**Krishna Khadka:**\n\nHi Group,\nMilestone:\n\n\n1. ABLE Global Surrogate Model\n\nAction Items:\n\n\n1. I have been doing literature review on the feasibility of ABLE global approach. There are existing work that construct global interpretable models from local interpretable model like LIME. I’m currently exploring in more depth — the technical challenges that are there to construct global model from local model, limitation of existing work, and specific risk pertaining to extending ABLE as a global approach.\n2.  Worked together with Shovon to better understanding his project and draft the approach section. I’m also reverifying and running the code.\n\nResults/Finding:\n\n\n1. The literature review shows existing work in constructing global methods from local methods. \n\nIssues:\n\n\n1. I don’t have outstanding issues at the moment.",
          "placeholder": false
        },
        {
          "date": "2025-12-05",
          "content": "**Krishna Khadka:**\n\nCan you add me @Fadul Sikder",
          "placeholder": false
        },
        {
          "date": "2025-10-28",
          "content": "**Krishna Khadka:**\n\nHi Group,\nMilestone:\nMilestone:\n\n\n1. CNN paper resubmission — Nov 13\n2. ABLE Global Surrogate Model\n3. Proposal\n\nAction Items:\n\n\n1. I completed the implementation of delta debugging on vision transformer models. I was able to get heat maps as explanation\n2. Some more items to include for validation:\n\n\n1. Smaller decision preserving sets results in better interpretability. Evaluate through a theoretical or a human evaluation for verification.\n2. Conduct an ablation study to verify that OFAT works best for linear heads while Delta Debugging is more effective for non-linear heads, by applying cross-validation between the two.\n\nResults/Finding:\n\n\n1. The results look interesting. The minimal feature patches identified are often **scattered and sparse**, suggesting room for structural refinement.\n2. Identified ViT-ReciproCAM (2024) as a relevant recent approach addressing sparse patch explanations and I plan to integrate similar logic to improve patch coherence.\n\nIssues:\n\n\n1. I'm revisiting the code and fix the sparsity issue.\n\n\nI will be unable to attend the meeting today from 12:30-2 pm as I’ll be conducting a midterm 2 exam for my TA class .\n\n**Replies:**\n\n**Krishna Khadka:**\nAn example of heatmap generation in a ViT architecture using DD-CAM.\n\n---\n\n**Krishna Khadka:**\nThis is similar to outputs from Relevance in the following picture, which ViTReciproCAM has tried to resolve.\n\n---",
          "placeholder": false
        }
      ]
    },
    {
      "slackId": "U0798MEJQRH",
      "name": "Pujan Budhathoki",
      "slug": "pujan-budhathoki",
      "photo": "pujan.png",
      "updates": [
        {
          "date": "2026-01-27",
          "content": "**Pujan Budhathoki:**\n\nStatus Update:\n**Milestone: Submit the Literature report(as discussed) on the Mechanistic Analysis of LLMs for Secure Code generation: 27th January **\n\n**Action Items: **\n\n\n1. Firstly, I have been going through literature/articles/tutorials to gain a deeper understanding of the concepts required to fully understand the problem. (i.e. Attention Mechanism, Mechanistic Interpretability, Sparse Autoencoder, etc) \n2. Secondly, I have been going through literature relevant to our problem. I have attached the literature review report I have so far,I will add more updates to it as I go through different literature and more in-depth literature reviews. \n3. I have been working with Saif, and as an action item, I have asked him to create a summarized report on the progress he made on the project so far and the reasoning behind the decisions. \n\n\n**Results/Findings: **\n\n\n1. I have summarized the findings in the report.\n\n\n**Questions/Issues:**\n\n\n1. I have no questions or issues at this time.",
          "placeholder": false
        },
        {
          "date": "2026-01-15",
          "content": "**Pujan Budhathoki:**\n\nHello everyone,\n**Milestone: Present on the Approach addressing the comments: 22nd January**\n**I was ill for most of the time last week so it wasn’t a very productive week for me**\n\n**Furthermore, as the new semester begins, I commit that I will post my progress status updates on my slack channel every two days (or everyday, whichever you think will be better) **\n**Action Items:**\n\n\n1. I have been going over the literature very carefully thinking of how I can position my paper. I have some very rough ideas, I will scrutinize them and see which ones work the best. \n\n**Results/Findings:**\n\n\n1. I do not have any notable findings at this point\n\n**Issues: **\n\n\n1. I do not have any issues or questions at this point\n\n**Replies:**\n\n**Dr. Lei:**\nGlad to see your commitment. Status updates for every two days is fine. I also encourage you to be critical at what you read and what you do, and ask questions, which is what research is about.\n\n---",
          "placeholder": false
        },
        {
          "date": "2026-01-08",
          "content": "**Pujan Budhathoki:**\n\nHello everyone,\n**Milestone: Present on the Approach addressing the comments**\n**I have been very ill and bedridden for the past few days, I will post a proper status update after I get better, on my slack channel. **\n**Action Items:**\n\n\n1. Over the break, I have been very thoroughly going through numerous literature surrounding Memorization and Memorization detection in machine learning:\n\n\n1. Baldock, Robert, Hartmut Maennel, and Behnam Neyshabur. \"Deep learning through the lens of example difficulty.\" Advances in Neural Information Processing Systems 34 (2021): 10876-10889.\n2. Chatterjee, Satrajit. \"Learning and memorization.\" International conference on machine learning. PMLR, 2018.\n3. Somvanshi, Shriyank, et al. \"A survey on deep tabular learning.\" arXiv preprint arXiv:2410.12034 (2024).\n4. Wei, Jiaheng, et al. \"Memorization in deep learning: A survey.\" ACM Computing Surveys 58.4 (2025): 1-35.\n5. Stephenson, Cory, et al. \"On the geometry of generalization and memorization in deep neural networks.\" arXiv preprint arXiv:2105.14602 (2021).\n6. Arpit, Devansh, et al. \"A closer look at memorization in deep networks.\" International conference on machine learning. PMLR, 2017.\n\n\n1. I have also been carrying out numerous experiments to try and standardize the differences between memorized and non-memorized points in terms of:\n\n\n1. How much change we can bring in prediction confidence of memorized vs generalized points under small perturbations\n2. How many features do we need to modify to bring a big change in confidence prediction of memorized vs generalized points \n\n**Results/Findings:**\n\n\n1. I do not have any notable findings at this point\n\n**Issues: **\n\n\n1. I do not have any issues or questions at this point",
          "placeholder": false
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-11",
          "content": "**Pujan Budhathoki:**\n\n**Hello everyone,**\n**Milestone: Present on the Approach addressing the comments**\n**This week I was fully busy in grading work and finalizing the students’ grades so I couldn’t get anything else done. Now that my TA work has ended, I will fully focus on research.**\n**Action Items for next week:**\n\n\n1. Go deep into the approach and see how we can make a paper out of it\n\n**Results/Findings:**\n\n\n1. I do not have any notable findings this week\n\n**Issues: **\n\n\n1. I do not have any issues or questions this week",
          "placeholder": false
        },
        {
          "date": "2025-12-05",
          "content": "**Pujan Budhathoki:**\n\n**Hello everyone,**\n**Milestone: Present on the Approach addressing the comments**\n**Last week I was out of town for Thanksgiving through the weekends. This week, I spent my time in my TA work(Giving tutoring sessions to students and helping them with their project and Grading) so I couldn’t commit time to research this week.**\n**Action Items:**\n\n\n1. I have been carrying out minor literature reviews and experiments\n\n**Results/Findings:**\n\n\n1. I do not have any notable findings this week\n\n**Issues:**\n\n\n1. I do not have any issues or questions this week\n\n**Replies:**\n\n**Dr. Lei:**\nI am seriously concerned about your research progress. You keep pushing your milestones, which is unacceptable.\n\n---\n\n**Pujan Budhathoki:**\nI understand Dr. Lei. My research progress has been unacceptable these past few weeks. I should have put in more effort to push the project further. I apologize for this. I also should’ve better balanced my research and my TA work and made better use of my time. I have no excuses for this. I make the commitment that as soon as I finish my TA grading work, I will fully push my project providing timely updates and much higher performance than before.\n\n---",
          "placeholder": false
        },
        {
          "date": "2025-10-28",
          "content": "**Pujan Budhathoki:**\n\nHello everyone,\n**Milestone: Get Experimental Results on discussed experiments**\n**Action Items Completed:**\n\n\n1. I ran several experiments this week to observe what we discussed\n\n\n**Findings: (More Detailed results given in my Slack Channel)**\n\n\n1. On closer inspection, I found that when the training loss on each of the outlier points are the lowest it can be (9.999778782803785e-13) (Training prediction confidence is 100%), Outlier points and generalized points behave the same way under perturbations. But when training loss of the outlier points is higher than this (in the range of e-10 to e-7)(Training prediction confidence of 99.99999%) , Outlier points behave in the way we theorized under perturbations.  I am investigating more on this.\n\n**Issues/Questions:**\n\n\n1. I have no issues this week. I am investigating more on why I’m getting the results I’m getting.",
          "placeholder": false
        }
      ]
    },
    {
      "slackId": "U079265G0ES",
      "name": "Qiping Wei",
      "slug": "qiping-wei",
      "photo": "qiping.jpg",
      "updates": [
        {
          "date": "2026-01-28",
          "content": "No update was provided for the week ending 2026-01-28.",
          "placeholder": true
        },
        {
          "date": "2026-01-21",
          "content": "No update was provided for the week ending 2026-01-21.",
          "placeholder": true
        },
        {
          "date": "2026-01-14",
          "content": "No update was provided for the week ending 2026-01-14.",
          "placeholder": true
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-17",
          "content": "No update was provided for the week ending 2025-12-17.",
          "placeholder": true
        },
        {
          "date": "2025-12-10",
          "content": "No update was provided for the week ending 2025-12-10.",
          "placeholder": true
        },
        {
          "date": "2025-11-03",
          "content": "No update was provided for the week ending 2025-11-03.",
          "placeholder": true
        }
      ]
    },
    {
      "slackId": "U08F6QCJ4JJ",
      "name": "Shovon Niverd",
      "slug": "shovon-niverd",
      "photo": "shovon_pereira.jpg",
      "updates": [
        {
          "date": "2026-01-22",
          "content": "**Shovon Niverd:**\n\nMilestone:\n  IJCAI Submission completed January 19\n Work on potential improvements on the last paper\n Work towards a new project simultaneously \nAction Items:\n Work on multi class datasets with TabKD\n While working on last project, we encountered some new areas of distillation; I will try to explore them and share.\nResults/Finding:\nNo findings yet as I have not worked after the submission, hopefully I will have something next week \nIssues:\nNone\n\n**Replies:**\n\n**Dr. Lei:**\nGood job on making the submission. As you are thinking about the next project, do not just put the paper aside yet. You need to identify things that you were not confident about but did not have time to address them before the deadline.\n\n---",
          "placeholder": false
        },
        {
          "date": "2026-01-15",
          "content": "**Shovon Niverd:**\n\n**Milestone**:\n IJCAI Submission January 19\n**Action Items**:\n Over the break we discussed on how to improve the approach in terms of training stability and remove the cyclic dependency.\n Completed experiments, which shows better results compared to 5 other baselines.\nThe first draft of the paper is ready, only the conclusion needs to be written\nOver the next few days we will improve the quality of writing, verifying the sections, and if possible add more experiments \nResults/Finding:\nThe current result shows our approach is better than 5 other baselines.\nIssues:\nNo big issues.\n\n**Replies:**\n\n**Dr. Lei:**\nSounds good. Just a reminder, a solid paper is not only about the results, but also about the rationale behind the approach, in terms of the design decisions you make and why you make them. This is something you will need to focus on the paper writing.\n\n---\n\n**Shovon Niverd:**\nThank you Dr Lei, we will focus on properly narrating our rationale behind each design decision in the next couple of days.\n\n---",
          "placeholder": false
        },
        {
          "date": "2026-01-14",
          "content": "No update was provided for the week ending 2026-01-14.",
          "placeholder": true
        },
        {
          "date": "2026-01-07",
          "content": "No update was provided for the week ending 2026-01-07.",
          "placeholder": true
        },
        {
          "date": "2025-12-31",
          "content": "No update was provided for the week ending 2025-12-31.",
          "placeholder": true
        },
        {
          "date": "2025-12-24",
          "content": "No update was provided for the week ending 2025-12-24.",
          "placeholder": true
        },
        {
          "date": "2025-12-11",
          "content": "**Shovon Niverd:**\n\n**Milestones:**\n\n\n1. Submit to IJCAI, January 19th, 2026\n\n**Action Items:**\n\n\n1. Study effect on bin variances due to losses introduced in bin learner.\n2. Study amount of samples per bin.\n3. Finalize the final approach.\n4. Prepare for further experiments.\n5. Also which previous works are comparable to our concept, this needs to be decided too.|\n6. Need to work on integrating categorical features in the architecture\n\n**Results/Findings:**\n\n\n1. So far proportion of samples in each bin almost evens out towards the end, this indicates the implemented diversity loss works.\n2. Intra-bin variance goes down, actually towards the middle it goes up and then comes down towards the end; but inter-bin does not really show significant change.\n3. More results are shared in my channel\n\nIssues/Question:\n\n\n1. Few features shows sign of collapse, they have all features inside one bin, every time it's the same three features.\n2. I am also working on integrating categorical features.",
          "placeholder": false
        },
        {
          "date": "2025-12-05",
          "content": "**Shovon Niverd:**\n\nwhich meeting are we joining?",
          "placeholder": false
        },
        {
          "date": "2025-10-28",
          "content": "**Shovon Niverd:**\n\n**Milestones:**\n\n\n1. Propose an approach for the new project related to interaction diversity(Oct 31).\n\n**Action Items:**\n\n\n1. Exploring literature related to feature interactions.\n2. Define how interaction diversity can be measured.\n3. Create a summary of how interaction diversity can be used to create a surrogate model.\n\n**Results/Findings:**\n\n\n1. A possible approach to a surrogate model with interaction diversity.\n\n\n1. Interaction diversity can be defined as a measure of *how many* feature pairs interact in a model.\n2. As a first step, we can consider having a black-box model with probability access only.\n3. Create a synthetic dataset using the t-way combinatorial approach. It will be a small dataset to start with\n4. As the trained black-box model has learned all the important interactions of the original dataset, it can give an indication of which feature interactions are actually important. Basically, we need to find out how many features or feature pairs interact with each other to enhance the model's performance. I haven't been able to bank on a particular method, which should be used to calculate this diversity.\n5. After the initial interaction measure is done, only those features or feature pairs that with enough importance will put through another t-way iteration to get more samples, thus iteratively creating a dataset for the surrogate model.\n6. Training the surrogate model with that dataset to get high fidelity.\n\n\n1. The following papers had interesting insights:\n\n\n1. [https://www.mdpi.com/2076-3417/9/23/5191](https://www.mdpi.com/2076-3417/9/23/5191)\n2. [https://ieeexplore.ieee.org/abstract/document/10675910](https://ieeexplore.ieee.org/abstract/document/10675910)\n\n**Issues**\n\n\n1. Need to decide what metric to use for the interaction diversity measure.\n2. Unsure of whether to use a GAN/similar generative model for synthetic data, or simply use a t-way test to generate a dataset. If GAN is used, how the interaction diversity will be represented as a feedback/loss is another challenge.\n\n**Replies:**\n\n**Dr. Lei:**\nThis is not really the direction I suggested. Try to think about this as a machine learning problem. Read the paper that uses class diversity, and see how they develop the approach. you just need to replace that with interaction diversity, and then ask what issues you need to address.\n\n---",
          "placeholder": false
        }
      ]
    }
  ],
  "last_updated": "2026-01-28T23:31:42.148497"
}